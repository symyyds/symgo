<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>小土堆pytorch</title>
</head>
<body><h2 id="v4YEb">两大法宝函数</h2>
<h3 id="PxcMY">dir()</h3>
<p>查看指定类/方法的函数目录(工具箱)</p>
<h3 id="xt25E">help()</h3>
<p>查看指定类/方法的使用帮助</p>
<h2 id="qdgQg">加载数据初认识</h2>
<h4 id="blmYA">Dataset&Dataloader</h4>
<ol start='' >
<li><p>Dataset：将数据进行整理并编号，同时提供一种方式去获取数据及其label；</p>
<ol start='' >
<li>如何获取每一个数据及其label；</li>
<li>总共有多少的数据；</li>

</ol>
</li>
<li><p>Dataloader：为后面的网络提供不同的数据形式；</p>
</li>

</ol>
<h4 id="yFk56">示例代码</h4>
<h5 id="Jhefd">目录结构</h5>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1649934705377-ff7d7540-bf9a-4c02-b787-da113b846e19.png" referrerpolicy="no-referrer"></p>
<h5 id="yU4tN">learn.py代码</h5>
<pre><code class='language-java' lang='java'>from torch.utils.data import Dataset

import os

from PIL import Image

root_train_dir = &quot;./dataset/train&quot;
root_val_dir = &quot;./dataset/val&quot;

label_bees = &quot;bees&quot;
label_ants = &quot;ants&quot;

class MyData(Dataset):

    def __init__(self, root_dir, label_dir):
        self.root_dir = root_dir
        self.label_dir = label_dir
        self.path = os.path.join(self.root_dir, self.label_dir)
        self.img_path_list = os.listdir(self.path)

    def __getitem__(self, item):
        img_name = self.img_path_list[item]
        img_path = os.path.join(self.path, img_name)
        image = Image.open(img_path)
        label = self.label_dir
        return image, label

    def __len__(self):
        return len(self.img_path_list)


if __name__ == &quot;__main__&quot;:
    myBees = MyData(root_dir=root_train_dir, label_dir=label_bees)
    

</code></pre>
<h5 id="G4DoM">数据集</h5>
<p>E:\项目文件\多模态情感识别\ccnu-deep-learning-code\python-restart\pytorch-learn\dataset</p>
<h4 id="OyTiB">os.path.join的坑</h4>
<p>对于下面的代码：</p>
<pre><code class='language-python' lang='python'>import os

path1 = &quot;D://python-learn&quot;
path2 = &quot;/train&quot;

print(os.path.join(path1, path2))

path3 = &quot;train&quot;

print(os.path.join(path1, path3))
</code></pre>
<p>最终打印的结果为如下:</p>
<blockquote><p>/train</p>
<p>D://python-learn/train</p>
</blockquote>
<p>原因：</p>
<ol start='' >
<li>如果各组件名开头不包含/，函数会自动加上</li>
<li>如果后面的组件开头为/，则函数会自动忽略该组件前面的所有组件</li>
<li>如果最后一个组件为空，则函数会自动以/结尾</li>

</ol>
<h2 id="vE1YO">TensorBoard的使用</h2>
<h2 id="oUTHm">Transforms的使用</h2>
<h3 id="nnWd6">所在包</h3>
<p>处于torchvision包下，引入方式：</p>
<blockquote><p>from torchvision import transforms</p>
</blockquote>
<h3 id="HwCMp">常用transforms</h3>
<h4 id="gCNv2">Totensor</h4>
<p>将PIL Image或者numpy.ndarray类型的image转换成为tensor image：</p>
<pre><code class='language-python' lang='python'>&quot;&quot;&quot;
Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor. This transform does not support torchscript.

Converts a PIL Image or numpy.ndarray (H x W x C) in the range
[0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]
if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)
    or if the numpy.ndarray has dtype = np.uint8
&quot;&quot;&quot;
def __call__(self, pic):
    ...
</code></pre>
<p>使用示例：</p>
<pre><code class='language-python' lang='python'>def transforms_toTensor(image):

    &quot;&quot;&quot;
    transform PIL Image to Tensor Image
    :param image: PIL Image type
    :return: Tensor Image
    &quot;&quot;&quot;

    # 将image转换成一个tensor类型的图片
    tensor_trans = transforms.ToTensor()
    tensor_image = tensor_trans(image)
    return tensor_image

if __name__ == &quot;__main__&quot;:

    myBees = MyData(root_dir=root_train_dir, label_dir=label_bees)
    image, _ = myBees[0]
    tensor_image = transforms_toTensor(image)
</code></pre>
<h4 id="UQs9p">Normalize</h4>
<p>将图片按照指定的均值和方差进行标准化：</p>
<pre><code class='language-python' lang='python'>&quot;&quot;&quot;
Normalize a tensor image with mean and standard deviation.
    This transform does not support PIL Image.
    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``
    channels, this transform will normalize each channel of the input
    ``torch.*Tensor`` i.e.,
    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``
&quot;&quot;&quot;
def __init__(self, mean, std, inplace=False):
    ...
</code></pre>
<p>使用示例：</p>
<pre><code class='language-python' lang='python'>def transform_normalize(image):

    &quot;&quot;&quot;
    transforms.Normalize()
    output[channel] = (input[channel] - mean[channel]) / std[channel]
    :param image: tensor image
    :return: tensor image
    &quot;&quot;&quot;

    tensor_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    norm_image = tensor_norm(image)
    return norm_image

if __name__ == &quot;__main__&quot;:

    myBees = MyData(root_dir=root_train_dir, label_dir=label_bees)
    image, _ = myBees[0]
    tensor_image = transforms_toTensor(image)
    norm_image = transform_normalize(tensor_image)
</code></pre>
<h4 id="YXKqz">Resize</h4>
<p>修改图片的尺寸大小，如果size参数为元组，则将图片修改至指定的尺寸；如果size参数是一个整数，则将短边缩放至size，宽高比保持不变：</p>
<pre><code class='language-python' lang='python'>&quot;&quot;&quot;
    Resize the input image to the given size.
    If the image is torch Tensor, it is expected
    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions

    .. warning::
        The output image might be different depending on its type: when downsampling, the interpolation of PIL images
        and tensors is slightly different, because PIL applies antialiasing. This may lead to significant differences
        in the performance of a network. Therefore, it is preferable to train and serve a model with the same input
        types.

    Args:
        size (sequence or int): Desired output size. If size is a sequence like
            (h, w), output size will be matched to this. If size is an int,
            smaller edge of the image will be matched to this number.
            i.e, if height &gt; width, then image will be rescaled to
            (size * height / width, size).

            .. note::
                In torchscript mode size as single int is not supported, use a sequence of length 1: ``[size, ]``.
        interpolation (InterpolationMode): Desired interpolation enum defined by
            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.
            If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` and
            ``InterpolationMode.BICUBIC`` are supported.
            For backward compatibility integer values (e.g. ``PIL.Image.NEAREST``) are still acceptable.
        max_size (int, optional): The maximum allowed for the longer edge of
            the resized image: if the longer edge of the image is greater
            than ``max_size`` after being resized according to ``size``, then
            the image is resized again so that the longer edge is equal to
            ``max_size``. As a result, ``size`` might be overruled, i.e the
            smaller edge may be shorter than ``size``. This is only supported
            if ``size`` is an int (or a sequence of length 1 in torchscript
            mode).
        antialias (bool, optional): antialias flag. If ``img`` is PIL Image, the flag is ignored and anti-alias
            is always used. If ``img`` is Tensor, the flag is False by default and can be set True for
            ``InterpolationMode.BILINEAR`` only mode.

            .. warning::
                There is no autodiff support for ``antialias=True`` option with input ``img`` as Tensor.

&quot;&quot;&quot;
def __init__(self, size, interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=None):
</code></pre>
<p>使用示例：</p>
<pre><code class='language-python' lang='python'>def transform_resize(image):

    &quot;&quot;&quot;
    resize the PIL Image
    :param image: PIL Image
    :return:
    &quot;&quot;&quot;

    print(&quot;origin size: &quot;, image.size)
    image_resize = transforms.Resize((512, 512))
    resize_image = image_resize(image)
    print(&quot;resize: &quot;, resize_image.size)
    return resize_image

if __name__ == &quot;__main__&quot;:

    myBees = MyData(root_dir=root_train_dir, label_dir=label_bees)
    image, _ = myBees[0]

    resize_image = transform_resize(image)
</code></pre>
<h4 id="SAkbx">Compose</h4>
<p>组合多个transforms</p>
<pre><code class='language-python' lang='python'>&quot;&quot;&quot;
    Composes several transforms together. This transform does not support torchscript.
    Please, see the note below.

    Args:
        transforms (list of ``Transform`` objects): list of transforms to compose.

    Example:
        &gt;&gt;&gt; transforms.Compose([
        &gt;&gt;&gt;     transforms.CenterCrop(10),
        &gt;&gt;&gt;     transforms.ToTensor(),
        &gt;&gt;&gt; ])

    .. note::
        In order to script the transformations, please use ``torch.nn.Sequential`` as below.

        &gt;&gt;&gt; transforms = torch.nn.Sequential(
        &gt;&gt;&gt;     transforms.CenterCrop(10),
        &gt;&gt;&gt;     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        &gt;&gt;&gt; )
        &gt;&gt;&gt; scripted_transforms = torch.jit.script(transforms)

        Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require
        `lambda` functions or ``PIL.Image``.

&quot;&quot;&quot;
def __init__(self, transforms):
    ...
</code></pre>
<p>使用示例：</p>
<pre><code class='language-python' lang='python'>def transform_compose(image):

    &quot;&quot;&quot;
    compose include variable transforms
    :param image:  PIL Image
    :return:
    &quot;&quot;&quot;

    trans_resize = transforms.Resize(512)
    trans_totensor = transforms.ToTensor()

    trans_compose = transforms.Compose([trans_resize, trans_totensor])
    return trans_compose(image)

if __name__ == &quot;__main__&quot;:

    myBees = MyData(root_dir=root_train_dir, label_dir=label_bees)
    image, _ = myBees[0]

    trans_image = transform_compose(image)
</code></pre>
<h2 id="CFuGa">torchvision的数据集使用</h2>
<h3 id="Ztrzp">CIFAR10</h3>
<h4 id="eq8Ut">示例代码</h4>
<pre><code class='language-python' lang='python'>import torchvision


def cifar10_data():
    &quot;&quot;&quot;
    十分类数据集，包含了：飞机、汽车、鸟类、猫、鹿、狗、蛙类、马 、船和卡车
    :return:
    &quot;&quot;&quot;

    train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;, train=True, download=True)
    test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;, train=True, download=True)

    &#39;&#39;&#39;
        Dataset CIFAR10
            Number of datapoints: 50000
            Root location: ./dataset
            Split: Train
    &#39;&#39;&#39;
    print(test_set)

    image, label = test_set[0]  # (&lt;PIL.Image.Image image mode=RGB size=32x32 at 0x24799CC6F70&gt;, 6)

    image.show()


if __name__ == &quot;__main__&quot;:
    cifar10_data()
</code></pre>
<h4 id="IFybg">test_set结构</h4>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1649992876516-ae6591fd-33f3-451e-9523-b3139b952159.png" referrerpolicy="no-referrer"></p>
<h2 id="hSxii">DataLoader的使用</h2>
<h3 id="eFh8N">示例代码</h3>
<pre><code class='language-python' lang='python'>import torchvision
from torch.utils.data import DataLoader


def cifar10_data():
    &quot;&quot;&quot;
    十分类数据集，包含了：飞机、汽车、鸟类、猫、鹿、狗、蛙类、马 、船和卡车
    :return:
    &quot;&quot;&quot;

    dataset_transforms = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor()
    ])

    test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;, train=False, download=True, transform=dataset_transforms)

    test_loader = DataLoader(dataset=test_set, batch_size=4, shuffle=False, num_workers=0)

    # 传统取法
    # image, target = test_set[0]
    # print(image.shape)  # torch.Size([3, 32, 32])
    # print(target)  # 3

    # dataloader取法
    for data in test_loader:
        image, target = data
        print(image.shape)  # torch.Size([4, 3, 32, 32])
        print(target)  # tensor([3, 8, 8, 0]) 四张图片的target组成一个一维数组
        break


if __name__ == &quot;__main__&quot;:
    cifar10_data()
</code></pre>
<h3 id="WEOjK">test_loader结构</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1649994012566-7a4528a7-d691-42cf-98f8-238ce5b531e6.png" referrerpolicy="no-referrer"></p>
<h2 id="ShMLy">神经网络基础</h2>
<h3 id="EAuf3">nn.Module</h3>
<p>定义一个神经网络模型</p>
<p>官方文档示例</p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650010874997-74646527-86c2-44ff-b5bf-71d67d716cd1.png" referrerpolicy="no-referrer"></p>
<p>简单测试用例</p>
<pre><code class='language-python' lang='python'>class TestModule(nn.Module):

    def __init__(self):
        super(TestModule, self).__init__()

    def forward(self, input):
        return input ** input


if __name__ == &quot;__main__&quot;:
    cifar10_data()

    module = TestModule()
    input = torch.tensor(4.0)
    output = module(input)
    print(output)
</code></pre>
<h3 id="LUFvV">nn.functional.conv2d</h3>
<h4 id="MWAgd">API定义</h4>
<pre><code class='language-python' lang='python'>torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
</code></pre>
<p>用于二维图像卷积操作，参数介绍：</p>
<ol start='' >
<li>input：需要做卷积操作的输入数据</li>
<li>weight：卷积核</li>
<li>bias：偏置，通常不用</li>
<li>stride：每次移动步长，默认移动1，可以为元组(sH, sW)或者整数</li>
<li>padding：长宽位置填充的长度，默认不填充，可以为元组(pH, pW)或者整数或者一个字符串(&quot;same&quot; | &quot;valid&quot;)</li>

</ol>
<h4 id="eVziV">示例代码</h4>
<pre><code class='language-python' lang='python'>def conv2d_test():
    input = torch.tensor([[1, 2, 0, 3, 1],
                          [0, 1, 2, 3, 1],
                          [1, 2, 1, 0, 0],
                          [5, 2, 3, 1, 1],
                          [2, 1, 0, 1, 1]])

    kernel = torch.tensor([[1, 2, 1],
                           [0, 1, 0],
                           [2, 1, 0]])

    input = torch.reshape(input, (1, 1, 5, 5))
    kernel = torch.reshape(kernel, (1, 1, 3, 3))

    return F.conv2d(input=input, weight=kernel, stride=1), \
           F.conv2d(input=input, weight=kernel, stride=2), \
           F.conv2d(input=input, weight=kernel, stride=1, padding=1)


if __name__ == &quot;__main__&quot;:

    &#39;&#39;&#39;
        (
        tensor(
        [[[[10, 12, 12],
          [18, 16, 16],
          [13,  9,  3]]]]
        ), 
        tensor(
        [[[[10, 12],
          [13,  3]]]]
        ), 
        tensor(
        [[[[ 1,  3,  4, 10,  8],
          [ 5, 10, 12, 12,  6],
          [ 7, 18, 16, 16,  8],
          [11, 13,  9,  3,  4],
          [14, 13,  9,  7,  4]]]]
        )
        )
    &#39;&#39;&#39;
    print(conv2d_test())
</code></pre>
<h3 id="KNf6L">nn.Conv2d</h3>
<h4 id="ML5Zt">API定义</h4>
<pre><code class='language-python' lang='python'>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#39;zeros&#39;, device=None, dtype=None)
</code></pre>
<p>参数介绍：</p>
<ol start='' >
<li>in_channels：输入数据通道数</li>
<li>out_channel：输出数据通道数，卷积层会生成对应数量的卷积核</li>
<li>kernel_size：卷积核大小</li>
<li>stride：卷积过程中移动步长</li>
<li>padding：填充</li>
<li><font style="color:#E8323C;">dilation：卷积核对应位的距离（不懂）默认1</font></li>
<li>group：基本用不到</li>
<li>bias：常年设置为true</li>
<li>padding_mode：当需要填充时，填充的模式</li>

</ol>
<h4 id="ISe2h">尺寸变换</h4>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650020757528-0af2d4d7-e9fb-4e4c-818d-46eee155a600.png" referrerpolicy="no-referrer"></p>
<h4 id="Gppr2">示例代码</h4>
<pre><code class='language-python' lang='python'>def nn_conv2d_test():
    
    # 加载数据
    test_data = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;, train=False, transform=torchvision.transforms.ToTensor(),
                                             download=True)
    test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)

    class module2(nn.Module):

        def __init__(self):
            super(module2, self).__init__()
            self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)

        def forward(self, input):
            return self.conv1(input)

    m = module2()
    step = 0
    for data in test_loader:
        imgs, _ = data

        # torch.Size([64, 3, 32, 32])
        # print(imgs.shape)
        output_image = m(imgs)
        # torch.Size([64, 6, 30, 30])
        # print(output_image.shape)

        writer = SummaryWriter(log_dir=&quot;logs&quot;)

        writer.add_images(&quot;input_image&quot;, imgs, step)

        # 因为输出的图像通道数为6，writer不知道该如何显示，所以需要变换通道
        output_image = torch.reshape(output_image, (-1, 3, 30, 30))
        writer.add_images(&quot;output_image&quot;, output_image, step)

        step += 1

        writer.close()

        break


if __name__ == &quot;__main__&quot;:
    nn_conv2d_test()
</code></pre>
<h3 id="mgFPN">  nn.MaxPool2d</h3>
<h4 id="fC9Df">API定义</h4>
<pre><code class='language-python' lang='python'>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
</code></pre>
<p>参数介绍：</p>
<ol start='' >
<li>kernel_size：卷积核大小</li>
<li>stride：移动步长，通常不需要设置</li>
<li>padding：填充大小</li>
<li>dilation：...</li>
<li>return_indices: ...</li>
<li>ceil_mode：当卷积核覆盖的位置不足时，是否需要进行最大值池化计算，默认为False</li>

</ol>
<h4 id="v4288">尺寸变换</h4>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650020739132-6af8d13e-72f5-4f60-bc00-30f3ffcab3c5.png" referrerpolicy="no-referrer"></p>
<h4 id="y5Gto">示例代码</h4>
<pre><code class='language-python' lang='python'>def nn_maxpool2d_test():

    input = torch.tensor([[1, 2, 0, 3, 1],
                          [0, 1, 2, 3, 1],
                          [1, 2, 1, 0, 0],
                          [5, 2, 3, 1, 1],
                          [2, 1, 0, 1, 1]], dtype=torch.float32)

    input = torch.reshape(input, (-1, 1, 5, 5))

    class module(nn.Module):

        def __init__(self):
            super(module, self).__init__()
            # ceil_mode：不能完全覆盖的区域是否也会进行maxpool
            self.maxpool1 = nn.MaxPool2d(kernel_size=3, ceil_mode=True)
            self.maxpool2 = nn.MaxPool2d(kernel_size=3, ceil_mode=False)

        def forward(self, input):
            return self.maxpool1(input), self.maxpool2(input)

    m = module()
    output1, output2 = m(input)
    print(output1)
    print(output2)


if __name__ == &quot;__main__&quot;:

    &#39;&#39;&#39;
        tensor([[[[2., 3.],
          [5., 1.]]]])
        tensor([[[[2.]]]])
    &#39;&#39;&#39;
    nn_maxpool2d_test()
</code></pre>
<h3 id="rNgHb">nn.Sequential</h3>
<h4 id="lNfsF">CIFAR10数据集分类的模型实例</h4>
<h5 id="gkpWz">网络图</h5>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650020370876-28d8ef3c-613e-4fe1-b5db-120239a21c9a.png" referrerpolicy="no-referrer"></p>
<h5 id="YWEmD">网络搭建</h5>
<pre><code class='language-python' lang='python'>def cifar10_model_test():

    class module(nn.Module):

        def __init__(self):
            
            super(module, self).__init__()

            self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)
            self.maxpool1 = nn.MaxPool2d(kernel_size=2)

            self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2)
            self.maxpool2 = nn.MaxPool2d(kernel_size=2)

            self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)
            self.maxpool3 = nn.MaxPool2d(kernel_size=2)

            self.flatten = nn.Flatten()  # 64 * 4 * 4
            self.line1 = nn.Linear(in_features=64*4*4, out_features=64)
            self.line2 = nn.Linear(in_features=64, out_features=10)
            self.softmax = nn.Softmax()

        def forward(self, input):

            input = self.maxpool1(self.conv1(input))
            input = self.maxpool2(self.conv2(input))
            input = self.maxpool3(self.conv3(input))
            input = self.flatten(input)
            input = self.line2(self.line1(input))
            return self.softmax(input)

    m = module()

    input = torch.ones((3, 3, 32, 32))

    output = m(input)

    print(output.shape)


if __name__ == &quot;__main__&quot;:

    cifar10_model_test()
</code></pre>
<h5 id="jFJav">使用Sequential</h5>
<pre><code class='language-python' lang='python'>def cifar10_model_test():

    # trans_tensor = torchvision.transforms.ToTensor()

    # 加载数据集
    # train_data = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;, train=True, transform=trans_tensor, download=True)

    class module(nn.Module):

        def __init__(self):
            
            super(module, self).__init__()

            self.sequential = nn.Sequential(
                nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2),
                nn.MaxPool2d(kernel_size=2),
                nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=2),
                nn.MaxPool2d(kernel_size=2),
                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),
                nn.MaxPool2d(kernel_size=2),
                nn.Flatten(),
                nn.Linear(in_features=64 * 4 * 4, out_features=64),
                nn.Linear(in_features=64, out_features=10)
            )
     
            self.softmax = nn.Softmax()

        def forward(self, input):
            return self.softmax(self.sequential(input))

    m = module()

    input = torch.ones((3, 3, 32, 32))

    output = m(input)

    print(output.shape)

    print(output)


if __name__ == &quot;__main__&quot;:

    cifar10_model_test()
</code></pre>
<h5 id="VU9Rb">可视化模型结构</h5>
<pre><code class='language-python' lang='python'>    m = module()

    input = torch.ones((3, 3, 32, 32))

    writer = SummaryWriter(log_dir=&quot;./logs&quot;)

    writer.add_graph(m, input)

    writer.close()
</code></pre>
<p>效果</p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650021737499-8c5e52f5-e68a-47b3-85d8-049423c34263.png" referrerpolicy="no-referrer"></p>
<h2 id="l18ry">修改已有模型</h2>
<h3 id="EdyCP">torchvision.models</h3>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650026023830-2fb13517-0f91-45db-8014-960b87db66af.png" referrerpolicy="no-referrer"></p>
<h3 id="lB0C8">修改VGG16</h3>
<p>因为vgg16是在ImageNet数据集上使用的，而该数据集是千分类数据集，其网络模型结构如下</p>
<pre><code class='language-python' lang='python'>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</code></pre>
<p>可以通过如下几种方式对VGG16模型进行修改，使其能够用于CIFAR10</p>
<pre><code class='language-python' lang='python'>def vgg_modify_test():

    vgg16 = torchvision.models.vgg16(pretrained=False)
    # vgg16_pretrain = torchvision.models.vgg16(pretrained=True)
    # print(vgg16)

    # 单独作为一个Sequential
    # vgg16.add_module(name=&quot;satisfy_cifar10&quot;, module=nn.Linear(in_features=1000, out_features=10))
    # print(vgg16)

    # 添加到指定的层次中，如classifier
    # vgg16.classifier.add_module(name=&quot;7&quot;, module=nn.Linear(in_features=1000, out_features=10))
    # print(vgg16)

    # 修改指定层中的指定位置的线性/非线性层
    vgg16.classifier[6] = nn.Linear(in_features=4096, out_features=10)
    print(vgg16)
</code></pre>
<h2 id="UGFYN">模型参数的保存与读取</h2>
<h3 id="h6ZcA">示例代码</h3>
<p>两种方式：</p>
<ol start='' >
<li>保存整个模型，加载的时候也是直接加载成一个模型对象</li>
<li>只保留模型的参数，加载的时候需要将自己将参数载入到模型中</li>

</ol>
<pre><code class='language-python' lang='python'>def model_save_and_load():

    vgg16 = torchvision.models.vgg16(pretrained=False)

    # 模型保存方式1  保留全部模型
    # torch.save(vgg16, &quot;./model_load/vgg16.pth&quot;)

    # 模型提取方式1
    # vgg16_load1 = torch.load(&quot;./model_load/vgg16.pth&quot;)

    # 模型保存方式2  只保存网络模型参数
    # torch.save(vgg16.state_dict(), &quot;./model_load/vgg16_2.pth&quot;)

    # 模型提取方式2
    vgg16_load2 = torch.load(&quot;./model_load/vgg16_2.pth&quot;)
    # print(vgg16_load2)
    # 模型参数还原
    vgg16.load_state_dict(vgg16_load2)



if __name__ == &quot;__main__&quot;:

    model_save_and_load()
</code></pre>
<p>推荐使用方式2，因为对于模型而言，有用的信息就是其参数</p>
<h3 id="X2aHI">陷阱</h3>
<p>当保存模型的时候，导入模型的类中一定要有对应模型的定义，或者引入对应模型的定义的模块也可以，否则会报错</p>
<h2 id="ZT5ck">计算测试结果的准确率</h2>
<h3 id="trthZ">示例代码</h3>
<pre><code class='language-python' lang='python'>def accuracy_calculate_test():

    pred = torch.tensor([[0.1,0.2],[0.3, 0.4], [0.6, 0.5]])
    target = torch.tensor([1, 0, 1])
    # print(pred.argmax(1))  # 横向比较找到最大的位置  tensor([1, 1])
    # print(pred.argmax(0))  # 纵向比较找到最大的位置  tensor([1, 1])
    pred = pred.argmax(1)
    print(pred == target)  # tensor([ True, False, False])
    print(pred.eq(target)) # tensor([ True, False, False])
    print(pred.eq(target).sum())  # True为1，False为0  结果为1，即正确预测的个数



if __name__ == &quot;__main__&quot;:

    accuracy_calculate_test()
</code></pre>
<h2 id="h00G1">GPU训练</h2>
<h3 id="UdGgD">简介</h3>
<p>需要修改三个地方：</p>
<ol start='' >
<li>模型对象</li>
<li>数据集（包括传入模型的输入数据 和 标签）</li>
<li>损失函数</li>

</ol>
<p>&nbsp;</p>
<p>修改方式：调用对应的.cuda()</p>
<p><img src="https://cdn.nlark.com/yuque/0/2022/png/21725882/1650027823156-53b3a427-56ce-4bec-baed-a42257701811.png" referrerpolicy="no-referrer"></p>
<h3 id="zaVa8">示例代码</h3>
<h4 id="GTEfj">方式一</h4>
<pre><code class='language-python' lang='python'># 将网络模型在gpu上训练
model = Model()
if torch.cuda.is_available():
	model = model.cuda()

# 损失函数在gpu上训练
loss_fn = nn.CrossEntropyLoss()
if torch.cuda.is_available():	
	loss_fn = loss_fn.cuda()

# 数据在gpu上训练
for data in dataloader:                        
	imgs, targets = data
    if torch.cuda.is_available():
        imgs = imgs.cuda()
        targets = targets.cuda()

</code></pre>
<h4 id="eOXzZ">方式二</h4>
<pre><code class='language-python' lang='python'>device = torch.device(&quot;cpu&quot;)	# 使用cpu训练
device = torch.device(&quot;cuda&quot;)	# 使用gpu训练 
device = torch.device(&quot;cuda:0&quot;)	# 当电脑中有多张显卡时，使用第一张显卡
device = torch.device(&quot;cuda:1&quot;)	# 当电脑中有多张显卡时，使用第二张显卡

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

model = model.to(device)

loss_fn = loss_fn.to(device)

for data in train_dataloader:
    imgs, targets = data
    imgs = imgs.to(device)
    targets = targets.to(device)
</code></pre>
<p>&nbsp;</p>
</body>
</html>